{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for analysing the results of the sandbox environmet. All cells can be executed at the start to generate insights into the results. The notebook starts with defining the functions, followed by their execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "#load results\n",
    "data = pd.read_json(\"../logs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>rounds</th>\n",
       "      <th>losses_distributed</th>\n",
       "      <th>number_of_clients</th>\n",
       "      <th>entries</th>\n",
       "      <th>number_of_samples</th>\n",
       "      <th>attributes</th>\n",
       "      <th>stations</th>\n",
       "      <th>scenario</th>\n",
       "      <th>percentage_of_testing_data</th>\n",
       "      <th>loss</th>\n",
       "      <th>epochs</th>\n",
       "      <th>hidden_layers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-24 22:10:17</td>\n",
       "      <td>MLP regressor</td>\n",
       "      <td>weather</td>\n",
       "      <td>1</td>\n",
       "      <td>[[1, 0.23393965120000002]]</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>100000</td>\n",
       "      <td>[temp]</td>\n",
       "      <td>[potsdam, muenchen]</td>\n",
       "      <td>separate</td>\n",
       "      <td>0.2</td>\n",
       "      <td>MAE</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date          model  dataset  rounds  \\\n",
       "0 2023-01-24 22:10:17  MLP regressor  weather       1   \n",
       "\n",
       "           losses_distributed  number_of_clients  entries  number_of_samples  \\\n",
       "0  [[1, 0.23393965120000002]]                  2       10             100000   \n",
       "\n",
       "  attributes             stations  scenario  percentage_of_testing_data loss  \\\n",
       "0     [temp]  [potsdam, muenchen]  separate                         0.2  MAE   \n",
       "\n",
       "   epochs  hidden_layers  \n",
       "0      10              1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis functions\n",
    "def rank_model_performance(dataset: str, model: str, metric: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame containing all test runs for `dataset` with `model` ranked by `metric`. \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def rank_all_models_performances(dataset: str, metric: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame containing all test runs for `dataset` ranked by `metric`. \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def compare_to_baseline(dataset: str, model: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Returns a tuple with the best `model` result for `dataset` and the baseline model.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def compare_to_distributed_scenario(dataset: str, model:str) -> tuple:\n",
    "    \"\"\"\n",
    "    Returns a tuple with the best `model` result for `dataset` and the distributed model.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def describe_model_performance(dataset: str, model: str, metric: str):\n",
    "    \"\"\"\n",
    "    Prints a description of `metric` for `model` and use case `dataset`.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "#plotting functions\n",
    "def plot_best_performances_barplot(dataset: str):\n",
    "    \"\"\"\n",
    "    Plot the best results for each ML algorithm for `dataset` as a barplot.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def plot_best_model_performance_lineplot(dataset: str, model: str):\n",
    "    \"\"\"\n",
    "    Plot the performance while training `model` on use case `dataset` as a lineplot.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
