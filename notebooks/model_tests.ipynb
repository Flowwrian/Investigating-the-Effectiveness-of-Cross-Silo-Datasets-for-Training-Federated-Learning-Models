{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/florian/bachelorarbeit/code/Cross-Silo-FL/datasets/vertical/covid/owid-covid-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso_code</th>\n",
       "      <th>continent</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_cases_smoothed</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>new_deaths_smoothed</th>\n",
       "      <th>...</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>life_expectancy</th>\n",
       "      <th>human_development_index</th>\n",
       "      <th>population</th>\n",
       "      <th>excess_mortality_cumulative_absolute</th>\n",
       "      <th>excess_mortality_cumulative</th>\n",
       "      <th>excess_mortality</th>\n",
       "      <th>excess_mortality_cumulative_per_million</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>41128772.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>41128772.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>41128772.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>41128772.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>41128772.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  iso_code continent     location        date  total_cases  new_cases  \\\n",
       "0      AFG      Asia  Afghanistan  2020-02-24          5.0        5.0   \n",
       "1      AFG      Asia  Afghanistan  2020-02-25          5.0        0.0   \n",
       "2      AFG      Asia  Afghanistan  2020-02-26          5.0        0.0   \n",
       "3      AFG      Asia  Afghanistan  2020-02-27          5.0        0.0   \n",
       "4      AFG      Asia  Afghanistan  2020-02-28          5.0        0.0   \n",
       "\n",
       "   new_cases_smoothed  total_deaths  new_deaths  new_deaths_smoothed  ...  \\\n",
       "0                 NaN           NaN         NaN                  NaN  ...   \n",
       "1                 NaN           NaN         NaN                  NaN  ...   \n",
       "2                 NaN           NaN         NaN                  NaN  ...   \n",
       "3                 NaN           NaN         NaN                  NaN  ...   \n",
       "4                 NaN           NaN         NaN                  NaN  ...   \n",
       "\n",
       "   male_smokers  handwashing_facilities  hospital_beds_per_thousand  \\\n",
       "0           NaN                  37.746                         0.5   \n",
       "1           NaN                  37.746                         0.5   \n",
       "2           NaN                  37.746                         0.5   \n",
       "3           NaN                  37.746                         0.5   \n",
       "4           NaN                  37.746                         0.5   \n",
       "\n",
       "   life_expectancy  human_development_index  population  \\\n",
       "0            64.83                    0.511  41128772.0   \n",
       "1            64.83                    0.511  41128772.0   \n",
       "2            64.83                    0.511  41128772.0   \n",
       "3            64.83                    0.511  41128772.0   \n",
       "4            64.83                    0.511  41128772.0   \n",
       "\n",
       "   excess_mortality_cumulative_absolute  excess_mortality_cumulative  \\\n",
       "0                                   NaN                          NaN   \n",
       "1                                   NaN                          NaN   \n",
       "2                                   NaN                          NaN   \n",
       "3                                   NaN                          NaN   \n",
       "4                                   NaN                          NaN   \n",
       "\n",
       "   excess_mortality  excess_mortality_cumulative_per_million  \n",
       "0               NaN                                      NaN  \n",
       "1               NaN                                      NaN  \n",
       "2               NaN                                      NaN  \n",
       "3               NaN                                      NaN  \n",
       "4               NaN                                      NaN  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "#loading covid data\n",
    "from sktime.forecasting.model_selection import SlidingWindowSplitter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def _get_samples_from_covid_data(n: int, attributes: list[str], num_of_samples: int, serialize: bool):\n",
    "    \"\"\"\n",
    "    Generates samples from the covid dataset.\n",
    "    Args:\n",
    "        n (int): Numbers of records per sample.\n",
    "        attributes (list[str]): List of attributes that will be used from the dataset. The fist element is the endogene variable.\n",
    "        num_of_samples (int): Number of returned samples.\n",
    "    \"\"\"\n",
    "\n",
    "    #load data\n",
    "    data = pd.read_csv(\"../datasets/vertical/covid/owid-covid-data.csv\")\n",
    "\n",
    "    #load data if already serialized\n",
    "    path = Path(f'../datasets/samples/covid_{n}_{\"_\".join(attributes)}.pkl')\n",
    "    if path.exists():\n",
    "        pkl_file = open(path, 'rb')\n",
    "        x_data, y_data = pickle.load(pkl_file)\n",
    "        pkl_file.close()\n",
    "        \n",
    "        if len(data.index) > num_of_samples:\n",
    "            return x_data[:num_of_samples], y_data[:num_of_samples]\n",
    "\n",
    "        else:\n",
    "            return x_data[:len(data.index) - 1], y_data[:len(data.index) - 1]\n",
    "\n",
    "    #fill nan with 0\n",
    "    data[attributes] = data[attributes].fillna(0)\n",
    "    data[\"new_cases\"] = data[\"new_cases\"].fillna(0)\n",
    "\n",
    "\n",
    "    # #scale the data\n",
    "    record_info = data[[\"iso_code\", \"continent\", \"location\", \"date\", \"tests_units\"]]\n",
    "\n",
    "    #scale selected attributes\n",
    "    selected_data = data[attributes]\n",
    "    selected_data_columns = selected_data.columns\n",
    "    scaler = StandardScaler()\n",
    "    scaled_selected_data = scaler.fit_transform(selected_data)\n",
    "    selected_data = pd.DataFrame(scaled_selected_data, columns=selected_data_columns)\n",
    "\n",
    "    #scale 'new_cases' (target)\n",
    "    if not \"new_cases\" in attributes:\n",
    "        new_cases_data = data[[\"new_cases\"]]\n",
    "        scaler = StandardScaler()\n",
    "        scaled_new_cases_data = scaler.fit_transform(new_cases_data)\n",
    "        new_cases_data = pd.DataFrame(scaled_new_cases_data, columns=[\"new_cases\"])\n",
    "\n",
    "        #combine scaled data\n",
    "        data = pd.concat([selected_data, new_cases_data, record_info], axis=1)\n",
    "\n",
    "    else:\n",
    "        data = pd.concat([selected_data, record_info], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    #split the data\n",
    "    countries = data.iso_code.drop_duplicates(keep=\"first\")\n",
    "    countries = countries[countries != \"ESH\"] #drop ESH because it only has one entry\n",
    "\n",
    "    for country in countries:\n",
    "        country_data = data[data.iso_code == country]\n",
    "\n",
    "        #generate input values\n",
    "        splitter = SlidingWindowSplitter(fh=1, window_length=n)\n",
    "        x_samples = splitter.split_series(country_data[attributes].to_numpy())\n",
    "\n",
    "        for sample in x_samples:\n",
    "            x_sample = sample[0].flatten()\n",
    "\n",
    "            if not np.isnan(np.sum(x_sample)): #check for nans\n",
    "                    x_data.append(x_sample)\n",
    "\n",
    "\n",
    "        #generate target values\n",
    "        y_samples = splitter.split_series(country_data[\"new_cases\"].to_numpy())\n",
    "\n",
    "        for sample in y_samples:\n",
    "            y_sample = sample[1].flatten()[0]\n",
    "\n",
    "            if not np.isnan(y_sample): #check for nans\n",
    "                y_data.append(y_sample)\n",
    "\n",
    "\n",
    "    #save data\n",
    "    if serialize:\n",
    "            output = open(path, \"wb\")\n",
    "            pickle.dump((x_data, y_data), output)\n",
    "            output.close()\n",
    "\n",
    "    #enusre that sample number is not out of range\n",
    "    if len(data.index) > num_of_samples:\n",
    "        return x_data[:num_of_samples], y_data[:num_of_samples]\n",
    "    else:\n",
    "        return x_data[:len(data.index) - 1], y_data[:len(data.index) - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = _get_samples_from_covid_data(10, [\"new_cases\"], 100000000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232956"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232956"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#load data\n",
    "data = []\n",
    "targets = []\n",
    "for attribute in [\"new_cases\", \"weekly_hosp_admissions\"]:\n",
    "    if targets == []:\n",
    "        X, y = helper.get_samples(\"covid\", 10, [attribute], \"\", True, max_samples=100000, standardize=True)\n",
    "        targets = tf.data.Dataset.from_tensor_slices(y).batch(1000)\n",
    "\n",
    "        tf_dataset = tf.data.Dataset.from_tensor_slices(X).batch(1000)\n",
    "        data.append(tf_dataset)\n",
    "\n",
    "    else:\n",
    "        X, _ = helper.get_samples(\"covid\", 10, [attribute], \"\", True, max_samples=100000, standardize=True)\n",
    "        tf_dataset = tf.data.Dataset.from_tensor_slices(X).batch(1000)\n",
    "        data.append(tf_dataset)\n",
    "\n",
    "#add targets as last entry\n",
    "data.append(targets)\n",
    "\n",
    "debug_var = data\n",
    "\n",
    "tf_dataset = tf.data.Dataset.zip(tuple(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.14054086 -0.14054086 -0.1405043  -0.14054086 -0.14054086 -0.14054086\n",
      " -0.1405043  -0.14054086 -0.14054086 -0.1405043  -0.14046773 -0.14047992\n",
      " -0.14052867 -0.14054086 -0.14054086 -0.14054086 -0.14054086 -0.14041897\n",
      " -0.14046773 -0.14051649 -0.1401508  -0.14046773 -0.14040678 -0.14035802\n",
      " -0.14044335 -0.14054086 -0.13990701 -0.14022394 -0.14001672 -0.14012642\n",
      " -0.14052867 -0.14018737 -0.14007767 -0.14017518 -0.13985826 -0.14028489\n",
      " -0.13960228 -0.14054086 -0.14012642 -0.13990701 -0.13983388 -0.13926098\n",
      " -0.14024832 -0.1399192  -0.13977293 -0.14023613 -0.13977293 -0.14017518\n",
      " -0.13973636 -0.13951695 -0.13993139 -0.13927316 -0.13891967 -0.13971198\n",
      " -0.13844428 -0.13902938 -0.14054086 -0.1363477  -0.13690842 -0.14054086\n",
      " -0.14054086 -0.14054086 -0.13133785 -0.13849304 -0.13845647 -0.14054086\n",
      " -0.13031394 -0.13734724 -0.13684747 -0.14054086 -0.1360064  -0.13183762\n",
      " -0.13628676 -0.13770073 -0.13521409 -0.13343443 -0.13456805 -0.13406828\n",
      " -0.13395858 -0.13156945 -0.13181324 -0.13429988 -0.13155726 -0.13386106\n",
      " -0.1327762  -0.13270307 -0.13098436 -0.13260555 -0.13254461 -0.13149632\n",
      " -0.13109406 -0.13293467 -0.12740067 -0.13344662 -0.13089903 -0.13353195\n",
      " -0.13682309 -0.1293266  -0.13142318 -0.13254461 -0.13376355 -0.13244709\n",
      " -0.13126472 -0.13924879 -0.1254138  -0.13489716 -0.13394639 -0.1354335\n",
      " -0.13400734 -0.13676214 -0.13642084 -0.13860275 -0.13401953 -0.13950476\n",
      " -0.13620143 -0.13626238 -0.13723753 -0.13906594 -0.13562853 -0.13737162\n",
      " -0.13685966 -0.13629895 -0.13714002 -0.13916346 -0.13664025 -0.13798109\n",
      " -0.13982169 -0.13394639 -0.13862713 -0.13932192 -0.14049211 -0.13706688\n",
      " -0.13810298 -0.13895624 -0.13860275 -0.1398095  -0.13827363 -0.14012642\n",
      " -0.13924879 -0.13917565 -0.13824925 -0.13973636 -0.13987045 -0.13906594\n",
      " -0.13924879 -0.13926098 -0.13928535 -0.13967542 -0.14054086 -0.13849304\n",
      " -0.14054086 -0.14054086 -0.14008986 -0.13954133 -0.13972417 -0.1400411\n",
      " -0.13959009 -0.14006548 -0.14054086 -0.13792014 -0.13961447 -0.1395779\n",
      " -0.14045554 -0.13907813 -0.13999234 -0.14054086 -0.1405043  -0.14054086\n",
      " -0.13859056 -0.13935849 -0.14007767 -0.13982169 -0.13998015 -0.13967542\n",
      " -0.14001672 -0.14034583 -0.14040678 -0.1405043  -0.14054086 -0.14030927\n",
      " -0.14012642 -0.13996796 -0.13999234 -0.14054086 -0.14034583 -0.13939506\n",
      " -0.13937068 -0.14022394 -0.14024832 -0.14019956 -0.14012642 -0.14011423\n",
      " -0.13962666 -0.13985826 -0.14001672 -0.14005329 -0.14033364 -0.14040678\n",
      " -0.14010205 -0.13901719 -0.14017518 -0.1402727  -0.13994358 -0.14023613\n",
      " -0.14034583 -0.14046773 -0.14011423 -0.14039459 -0.14035802 -0.14037021\n",
      " -0.14033364 -0.14047992 -0.14045554 -0.14000453 -0.13955352 -0.13976074\n",
      " -0.13978512 -0.13971198 -0.13960228 -0.14041897 -0.13937068 -0.13967542\n",
      " -0.13983388 -0.13973636 -0.1401508  -0.13996796 -0.13971198 -0.13982169\n",
      " -0.13948038 -0.1394682  -0.14054086 -0.13994358 -0.13912689 -0.13979731\n",
      " -0.13955352 -0.13974855 -0.13927316 -0.13938287 -0.13916346 -0.13904157\n",
      " -0.13973636 -0.13943163 -0.13961447 -0.13893186 -0.13938287 -0.13949257\n",
      " -0.13906594 -0.14005329 -0.13983388 -0.139005   -0.13885872 -0.13851742\n",
      " -0.13876121 -0.13827363 -0.13841991 -0.13973636 -0.13804203 -0.13855399\n",
      " -0.13779825 -0.13782262 -0.13594545 -0.13792014 -0.1398095  -0.13806641\n",
      " -0.13710345 -0.13700593 -0.13795671 -0.13778606 -0.13904157 -0.13899281\n",
      " -0.1375057  -0.13709126 -0.13787138 -0.13733505 -0.13715221 -0.13895624\n",
      " -0.13745694 -0.13737162 -0.13767635 -0.1377373  -0.13841991 -0.13794452\n",
      " -0.13965104 -0.139005   -0.13662806 -0.1379689  -0.13787138 -0.13781043\n",
      " -0.13718877 -0.13744475 -0.13956571 -0.13880997 -0.13798109 -0.13832239\n",
      " -0.1370425  -0.13763978 -0.13848085 -0.13951695 -0.13860275 -0.13883435\n",
      " -0.1383102  -0.14054086 -0.1383102  -0.13965104 -0.13904157 -0.13810298\n",
      " -0.13929754 -0.13939506 -0.13929754 -0.13901719 -0.13971198 -0.13945601\n",
      " -0.13994358 -0.13998015 -0.13924879 -0.13950476 -0.13985826 -0.1392366\n",
      " -0.13998015 -0.13959009 -0.1395779  -0.13887091 -0.13901719 -0.13956571\n",
      " -0.13961447 -0.14010205 -0.13960228 -0.13959009 -0.13927316 -0.14008986\n",
      " -0.13995577 -0.13969979 -0.14035802 -0.14010205 -0.13978512 -0.13989483\n",
      " -0.13984607 -0.14012642 -0.13974855 -0.14047992 -0.14024832 -0.14023613\n",
      " -0.14032145 -0.14032145 -0.14023613 -0.14019956 -0.14030927 -0.1402727\n",
      " -0.14049211 -0.1402727  -0.14033364 -0.14032145 -0.14047992 -0.14024832\n",
      " -0.1403824  -0.14018737 -0.14032145 -0.14034583 -0.14034583 -0.14040678\n",
      " -0.14045554 -0.14030927 -0.14022394 -0.14040678 -0.14047992 -0.13990701\n",
      " -0.1403824  -0.14045554 -0.14018737 -0.14054086 -0.14032145 -0.14026051\n",
      " -0.14002891 -0.14054086 -0.14022394 -0.14054086 -0.14041897 -0.14028489\n",
      " -0.14019956 -0.14023613 -0.14024832 -0.14041897 -0.13993139 -0.14024832\n",
      " -0.14035802 -0.14012642 -0.14019956 -0.14010205 -0.14049211 -0.14019956\n",
      " -0.13978512 -0.1396876  -0.13977293 -0.13987045 -0.14026051 -0.13955352\n",
      " -0.1400411  -0.13978512 -0.13939506 -0.1396876  -0.13961447 -0.13901719\n",
      " -0.14034583 -0.13954133 -0.13905376 -0.13898062 -0.14002891 -0.13959009\n",
      " -0.13921222 -0.13966323 -0.13926098 -0.13884654 -0.13838334 -0.1393463\n",
      " -0.1377373  -0.13824925 -0.13916346 -0.13844428 -0.13798109 -0.1387734\n",
      " -0.13802984 -0.13848085 -0.13817612 -0.1383102  -0.13837115 -0.13733505\n",
      " -0.13768854 -0.13609173 -0.13696936 -0.13688404 -0.13948038 -0.137847\n",
      " -0.13639646 -0.1367012  -0.13655492 -0.13676214 -0.13984607 -0.13966323\n",
      " -0.13915127 -0.137847   -0.13684747 -0.13501906 -0.13438521 -0.13559196\n",
      " -0.13759102 -0.13387325 -0.13288591 -0.13030175 -0.13294686 -0.13122815\n",
      " -0.12858304 -0.13261774 -0.12741286 -0.12665712 -0.12775416 -0.12721783\n",
      " -0.12214703 -0.12426799 -0.12517001 -0.12083057 -0.12243958 -0.11952631\n",
      " -0.11808795 -0.11830736 -0.1280589  -0.12687653 -0.12107436 -0.11842926\n",
      " -0.14034583 -0.10101058 -0.11368757 -0.11875837 -0.12365852 -0.11621078\n",
      " -0.11802701 -0.11362663 -0.12134253 -0.11543066 -0.12186667 -0.12718126\n",
      " -0.12187886 -0.12339035 -0.12224455 -0.12156194 -0.11689339 -0.12434113\n",
      " -0.12514563 -0.12604765 -0.12192762 -0.12156194 -0.12723002 -0.12258585\n",
      " -0.12602327 -0.13077714 -0.12743724 -0.12547474 -0.13135004 -0.12579167\n",
      " -0.12593794 -0.12624268 -0.13593326 -0.12960696 -0.12938755 -0.13629895\n",
      " -0.13540912 -0.13742037 -0.13777387 -0.13804203 -0.13549444 -0.13172791\n",
      " -0.13390982 -0.13512876 -0.13411704 -0.13284934 -0.13631114 -0.13464119\n",
      " -0.13338568 -0.13614048 -0.13532379 -0.13506782 -0.13529941 -0.13788357\n",
      " -0.13664025 -0.13767635 -0.13715221 -0.13722534 -0.13801765 -0.13845647\n",
      " -0.13941944 -0.13921222 -0.13930973 -0.13907813 -0.13995577 -0.14008986\n",
      " -0.13994358 -0.14037021 -0.13966323 -0.13960228 -0.13978512 -0.13932192\n",
      " -0.13885872 -0.13996796 -0.14022394 -0.13913908 -0.13966323 -0.14005329\n",
      " -0.13998015 -0.13969979 -0.14029708 -0.14019956 -0.13918784 -0.13941944\n",
      " -0.13920003 -0.13927316 -0.13905376 -0.14029708 -0.14044335 -0.13927316\n",
      " -0.13949257 -0.13928535 -0.13959009 -0.139005   -0.14054086 -0.14054086\n",
      " -0.1393463  -0.13899281 -0.13999234 -0.14001672 -0.13859056 -0.14054086\n",
      " -0.14054086 -0.13917565 -0.14028489 -0.14011423 -0.13998015 -0.14033364\n",
      " -0.14054086 -0.14054086 -0.13937068 -0.1402727  -0.13967542 -0.13994358\n",
      " -0.14030927 -0.14032145 -0.14002891 -0.1401508  -0.13982169 -0.14019956\n",
      " -0.13987045 -0.14046773 -0.1399192  -0.14023613 -0.14039459 -0.14023613\n",
      " -0.13983388 -0.1401508  -0.14005329 -0.14043116 -0.14049211 -0.13937068\n",
      " -0.14016299 -0.13989483 -0.14002891 -0.14017518 -0.14037021 -0.14005329\n",
      " -0.14012642 -0.14026051 -0.14034583 -0.14005329 -0.14018737 -0.14047992\n",
      " -0.14054086 -0.14054086 -0.14054086 -0.14033364 -0.14002891 -0.14016299\n",
      " -0.14026051 -0.14002891 -0.13983388 -0.14006548 -0.13944382 -0.14054086\n",
      " -0.13965104 -0.13990701 -0.1401508  -0.14035802 -0.13927316 -0.14033364\n",
      " -0.13917565 -0.14021175 -0.14030927 -0.14019956 -0.14002891 -0.14018737\n",
      " -0.1396876  -0.14019956 -0.14023613 -0.14030927 -0.14026051 -0.13999234\n",
      " -0.14043116 -0.14012642 -0.14001672 -0.14032145 -0.14044335 -0.1402727\n",
      " -0.14035802 -0.14039459 -0.14047992 -0.1398095  -0.14043116 -0.14040678\n",
      " -0.14002891 -0.14041897 -0.14030927 -0.14023613 -0.14008986 -0.14043116\n",
      " -0.14044335 -0.13985826 -0.14034583 -0.14016299 -0.14006548 -0.14030927\n",
      " -0.14019956 -0.14026051 -0.13954133 -0.14054086 -0.1402727  -0.14005329\n",
      " -0.14017518 -0.14023613 -0.14043116 -0.13966323 -0.1403824  -0.13960228\n",
      " -0.14005329 -0.13943163 -0.14008986 -0.14006548 -0.14006548 -0.13921222\n",
      " -0.13873683 -0.13937068 -0.13770073 -0.13794452 -0.1401508  -0.13930973\n",
      " -0.13753008 -0.13620143 -0.13517752 -0.13673777 -0.13989483 -0.13770073\n",
      " -0.13053335 -0.13060649 -0.13287372 -0.13280058 -0.13399515 -0.13284934\n",
      " -0.136238   -0.13468994 -0.13160602 -0.13060649 -0.13065525 -0.12959477\n",
      " -0.13454367 -0.13795671 -0.13503125 -0.13271526 -0.13839553 -0.13935849\n",
      " -0.13866369 -0.13829801 -0.13963885 -0.13720096 -0.13766416 -0.13718877\n",
      " -0.13828582 -0.13876121 -0.14008986 -0.13978512 -0.1375057  -0.13732286\n",
      " -0.13785919 -0.13817612 -0.13882216 -0.14054086 -0.1391147  -0.13748132\n",
      " -0.13544569 -0.136238   -0.13844428 -0.13605516 -0.13955352 -0.13974855\n",
      " -0.13856618 -0.13800547 -0.13856618 -0.13844428 -0.13840772 -0.13974855\n",
      " -0.13985826 -0.13988264 -0.1393463  -0.13976074 -0.13973636 -0.14054086\n",
      " -0.14054086 -0.14054086 -0.13811517 -0.13954133 -0.13985826 -0.13983388\n",
      " -0.14016299 -0.14011423 -0.14028489 -0.14024832 -0.1396876  -0.14011423\n",
      " -0.14002891 -0.14054086 -0.14054086 -0.14054086 -0.14054086 -0.13850523\n",
      " -0.13912689 -0.14007767 -0.13984607 -0.14028489 -0.14037021 -0.14016299\n",
      " -0.14006548 -0.13985826 -0.13979731 -0.14008986 -0.14021175 -0.14041897\n",
      " -0.1400411  -0.13985826 -0.14024832 -0.14005329 -0.1400411  -0.14026051\n",
      " -0.14046773 -0.14029708 -0.14051649 -0.14054086 -0.14054086 -0.14049211\n",
      " -0.14037021 -0.1405043  -0.13982169 -0.14018737 -0.14045554 -0.13915127\n",
      " -0.14007767 -0.14012642 -0.14006548 -0.14023613 -0.13988264 -0.14045554\n",
      " -0.13872464 -0.13907813 -0.14021175 -0.13993139 -0.14002891 -0.14054086\n",
      " -0.13987045 -0.13976074 -0.14054086 -0.13748132 -0.14010205 -0.13990701\n",
      " -0.13950476 -0.1394682  -0.13966323 -0.13930973 -0.13976074 -0.14016299\n",
      " -0.14054086 -0.13965104 -0.13989483 -0.14001672 -0.13956571 -0.14054086\n",
      " -0.14054086 -0.14054086 -0.13742037 -0.13983388 -0.13983388 -0.13774949\n",
      " -0.13969979 -0.14005329 -0.13941944 -0.13982169 -0.13952914 -0.13927316\n",
      " -0.13962666 -0.13998015 -0.14006548 -0.13960228 -0.1395779  -0.13937068\n",
      " -0.1395779  -0.13901719 -0.13973636 -0.13994358 -0.13955352 -0.13969979\n",
      " -0.14054086 -0.13827363 -0.13926098 -0.13887091 -0.14037021 -0.14017518\n",
      " -0.1405043  -0.14049211 -0.1403824  -0.13965104 -0.13994358 -0.14007767\n",
      " -0.13899281 -0.13913908 -0.137847   -0.13895624 -0.13827363 -0.1388831\n",
      " -0.13916346 -0.13915127 -0.13771292 -0.13728629 -0.13827363 -0.13906594\n",
      " -0.1394682  -0.13967542 -0.13813955 -0.13833458 -0.13822487 -0.13721315\n",
      " -0.13683528 -0.13639646 -0.13966323 -0.13648179 -0.13757884 -0.13711564\n",
      " -0.13766416 -0.13683528 -0.13812736 -0.13912689 -0.13779825 -0.13690842\n",
      " -0.13890748 -0.13770073 -0.13688404 -0.13756665 -0.13833458 -0.13800547\n",
      " -0.13570166 -0.13801765 -0.13642084 -0.13588451 -0.14054086 -0.14054086\n",
      " -0.13449491 -0.13514095 -0.1393463  -0.13754227 -0.13724972 -0.14054086\n",
      " -0.13576261 -0.13748132 -0.1382005  -0.1373838  -0.13568948 -0.13705469\n",
      " -0.13843209 -0.13859056 -0.13695718 -0.13740818 -0.13783481 -0.13631114\n",
      " -0.13909032 -0.13905376 -0.13963885 -0.13841991 -0.13817612 -0.13841991\n",
      " -0.13834677 -0.13767635 -0.13883435 -0.13955352 -0.13844428 -0.13899281\n",
      " -0.13801765 -0.139005   -0.13821269 -0.13906594 -0.13905376 -0.13961447\n",
      " -0.13860275 -0.1387734  -0.1386515  -0.13872464 -0.1388831  -0.13966323\n",
      " -0.13846866 -0.13935849 -0.13862713 -0.13928535 -0.1391147  -0.13849304\n",
      " -0.13954133 -0.13912689 -0.13976074 -0.13777387 -0.13988264 -0.13818831\n",
      " -0.13811517 -0.13960228 -0.13954133 -0.13943163 -0.13873683 -0.13856618\n",
      " -0.13933411 -0.13873683 -0.13959009 -0.13893186 -0.13935849 -0.13927316\n",
      " -0.1393463  -0.13895624 -0.13929754 -0.13960228 -0.1392366  -0.13873683\n",
      " -0.13916346 -0.13868807 -0.13818831 -0.13926098 -0.14023613 -0.13940725\n",
      " -0.13932192 -0.13915127 -0.13937068 -0.14054086 -0.14054086 -0.14052867\n",
      " -0.14054086 -0.14054086 -0.14052867 -0.14054086 -0.14052867 -0.14049211\n",
      " -0.1405043  -0.14041897 -0.1405043  -0.14030927], shape=(1000,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "i = True\n",
    "for batch in tf_dataset:\n",
    "    if i:\n",
    "        print(batch[-1])\n",
    "        i = False\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Weather data starts here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>dwpt</th>\n",
       "      <th>rhum</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>wpgt</th>\n",
       "      <th>pres</th>\n",
       "      <th>tsun</th>\n",
       "      <th>coco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16515.000000</td>\n",
       "      <td>16513.000000</td>\n",
       "      <td>16514.000000</td>\n",
       "      <td>11987.000000</td>\n",
       "      <td>4117.000000</td>\n",
       "      <td>10598.000000</td>\n",
       "      <td>11034.000000</td>\n",
       "      <td>8395.000000</td>\n",
       "      <td>11936.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.608653</td>\n",
       "      <td>7.365076</td>\n",
       "      <td>14.120358</td>\n",
       "      <td>1.579361</td>\n",
       "      <td>5.176099</td>\n",
       "      <td>208.351670</td>\n",
       "      <td>28.778204</td>\n",
       "      <td>53.423204</td>\n",
       "      <td>1010.997009</td>\n",
       "      <td>15.483871</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.861329</td>\n",
       "      <td>6.997318</td>\n",
       "      <td>8.975399</td>\n",
       "      <td>3.642694</td>\n",
       "      <td>23.663484</td>\n",
       "      <td>93.444179</td>\n",
       "      <td>15.034219</td>\n",
       "      <td>21.768710</td>\n",
       "      <td>9.549732</td>\n",
       "      <td>60.651942</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-17.600000</td>\n",
       "      <td>-18.700000</td>\n",
       "      <td>-14.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>961.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>16.600000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1005.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.600000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>26.600000</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>1011.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.800000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>38.900000</td>\n",
       "      <td>67.700000</td>\n",
       "      <td>1017.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>31.100000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>60.400000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>115.200000</td>\n",
       "      <td>154.800000</td>\n",
       "      <td>1044.800000</td>\n",
       "      <td>276.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               temp          dwpt          rhum          prcp         snow  \\\n",
       "count  16515.000000  16513.000000  16514.000000  11987.000000  4117.000000   \n",
       "mean      10.608653      7.365076     14.120358      1.579361     5.176099   \n",
       "std        7.861329      6.997318      8.975399      3.642694    23.663484   \n",
       "min      -17.600000    -18.700000    -14.900000      0.000000     0.000000   \n",
       "25%        4.600000      2.200000      7.100000      0.000000     0.000000   \n",
       "50%       10.600000      7.400000     14.100000      0.000000     0.000000   \n",
       "75%       16.800000     13.000000     21.100000      1.500000     0.000000   \n",
       "max       31.100000     24.700000     38.000000     60.400000   310.000000   \n",
       "\n",
       "               wdir          wspd         wpgt          pres        tsun  coco  \n",
       "count  10598.000000  11034.000000  8395.000000  11936.000000   31.000000   0.0  \n",
       "mean     208.351670     28.778204    53.423204   1010.997009   15.483871   NaN  \n",
       "std       93.444179     15.034219    21.768710      9.549732   60.651942   NaN  \n",
       "min        0.000000      0.000000     0.000000    961.300000    0.000000   NaN  \n",
       "25%      131.000000     16.600000    36.000000   1005.200000    0.000000   NaN  \n",
       "50%      235.000000     26.600000    51.500000   1011.300000    0.000000   NaN  \n",
       "75%      280.000000     38.900000    67.700000   1017.200000    0.000000   NaN  \n",
       "max      360.000000    115.200000   154.800000   1044.800000  276.000000   NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/home/florian/bachelorarbeit/code/Cross-Silo-FL/datasets/horizontal/weather/berlin_alexanderplatz.csv\", names=[\"time\", \"temp\", \"dwpt\", \"rhum\", \"prcp\", \"snow\", \"wdir\", \"wspd\", \"wpgt\", \"pres\", \"tsun\", \"coco\"])\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-12 14:48:59.003338: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-12 14:49:00.324999: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-12 14:49:00.325413: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-12 14:49:00.325425: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../scripts')\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "berlin_alexanderplatz\n",
      "y_test: False\n",
      "X_test: False\n",
      "frankfurt_am_main_westend\n",
      "y_test: False\n",
      "X_test: False\n",
      "hamburg_airport\n",
      "y_test: False\n",
      "X_test: False\n",
      "leipzig\n",
      "y_test: False\n",
      "X_test: False\n",
      "muenchen\n",
      "y_test: False\n",
      "X_test: False\n",
      "potsdam\n",
      "y_test: False\n",
      "X_test: False\n",
      "hannover\n",
      "y_test: False\n",
      "X_test: False\n",
      "koeln_bonn_airport\n",
      "y_test: False\n",
      "X_test: False\n",
      "stuttgart_schnarrenberg\n",
      "y_test: False\n",
      "X_test: False\n",
      "weimar\n",
      "y_test: False\n",
      "X_test: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#load test dataset\n",
    "all_stations = [\"berlin_alexanderplatz\", \"frankfurt_am_main_westend\", \"hamburg_airport\", \"leipzig\", \"muenchen\", \"potsdam\", \"hannover\", \"koeln_bonn_airport\", \"stuttgart_schnarrenberg\", \"weimar\"]\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for station in all_stations:\n",
    "    X, y = helper.get_samples(\n",
    "            \"weather\", 10, [\"temp\", \"dwpt\", \"rhum\"], station, False, 10000, True)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_test_list.append(X_test)\n",
    "    y_test_list.append(y_test)\n",
    "\n",
    "    print(station)\n",
    "    print(f\"y_test: {np.isnan(np.sum(y_test))}\")\n",
    "    print(f\"X_test: {np.isnan(np.sum(X_test))}\")\n",
    "X_test_list = np.array(X_test_list).flatten().reshape((20000, 30))\n",
    "y_test_list = np.array(y_test_list).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "test_dataset_weather = (X_test_list, y_test_list)\n",
    "output = open(\"../datasets/horizontal/weather/weather_test_dataset.pkl\", \"wb\")\n",
    "pickle.dump(test_dataset_weather, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.model_selection import SlidingWindowSplitter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _get_samples_from_weather_data(n: int, attributes: list[str], station: str, num_of_samples: int):\n",
    "    \"\"\"\n",
    "    Generates samples from the covid dataset.\n",
    "    Args:\n",
    "        n (int): Numbers of records per sample.\n",
    "        attributes (list[str]): List of attributes that will be used from the dataset. The fist element is the endogene variable.\n",
    "        station (str): Name of the station.\n",
    "        num_of_samples (int): Number of returned samples.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    #load data\n",
    "    path = f\"/home/florian/bachelorarbeit/code/Cross-Silo-FL/datasets/horizontal/weather/{station}.csv\"\n",
    "    data = pd.read_csv(path, names=[\"time\", \"temp\", \"dwpt\", \"rhum\", \"prcp\", \"snow\", \"wdir\", \"wspd\", \"wpgt\", \"pres\", \"tsun\", \"coco\"])\n",
    "    \n",
    "    #scale the data\n",
    "    # data = data.drop(\"time\", axis=1)\n",
    "    # data_columns = data.columns\n",
    "    # scaler = StandardScaler()\n",
    "    # scaled_data = scaler.fit_transform(data)\n",
    "    # data = pd.DataFrame(scaled_data, columns=data_columns)\n",
    "\n",
    "    #split the data\n",
    "    splitter = SlidingWindowSplitter(fh=1, window_length=n)\n",
    "    samples = splitter.split_series(data[attributes].to_numpy())\n",
    "\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    for sample in samples:\n",
    "        x_sample = sample[0].flatten()\n",
    "        y_sample = sample[1].flatten()[0] #the endogene temperature variable\n",
    "\n",
    "        if not np.isnan(np.sum(x_sample)) and not np.isnan(y_sample): #check for nans\n",
    "            x_data.append(x_sample)\n",
    "            y_data.append(y_sample)\n",
    "\n",
    "    return x_data[:num_of_samples], y_data[:num_of_samples]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = _get_samples_from_weather_data(10, [\"temp\"], \"berlin_alexanderplatz\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   0.972</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.972</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>3.410e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 02 Feb 2023</td> <th>  Prob (F-statistic):</th>           <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:32:59</td>     <th>  Log-Likelihood:    </th>          <td> -22019.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 10000</td>      <th>  AIC:               </th>          <td>4.406e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  9990</td>      <th>  BIC:               </th>          <td>4.413e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>  <td>    0.0538</td> <td>    0.010</td> <td>    5.392</td> <td> 0.000</td> <td>    0.034</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>  <td>   -0.0071</td> <td>    0.014</td> <td>   -0.492</td> <td> 0.623</td> <td>   -0.035</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>  <td>    0.0265</td> <td>    0.015</td> <td>    1.799</td> <td> 0.072</td> <td>   -0.002</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>  <td>    0.0045</td> <td>    0.015</td> <td>    0.304</td> <td> 0.761</td> <td>   -0.024</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>  <td>    0.0253</td> <td>    0.015</td> <td>    1.716</td> <td> 0.086</td> <td>   -0.004</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>  <td>    0.0019</td> <td>    0.015</td> <td>    0.127</td> <td> 0.899</td> <td>   -0.027</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>  <td>    0.0140</td> <td>    0.015</td> <td>    0.949</td> <td> 0.343</td> <td>   -0.015</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>  <td>    0.1031</td> <td>    0.015</td> <td>    7.003</td> <td> 0.000</td> <td>    0.074</td> <td>    0.132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>  <td>   -0.2769</td> <td>    0.014</td> <td>  -19.140</td> <td> 0.000</td> <td>   -0.305</td> <td>   -0.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th> <td>    1.0471</td> <td>    0.010</td> <td>  104.831</td> <td> 0.000</td> <td>    1.028</td> <td>    1.067</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>166.351</td> <th>  Durbin-Watson:     </th> <td>   2.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 329.485</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.035</td>  <th>  Prob(JB):          </th> <td>2.84e-72</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.886</td>  <th>  Cond. No.          </th> <td>    42.8</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                      y   R-squared (uncentered):                   0.972\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.972\n",
       "Method:                 Least Squares   F-statistic:                          3.410e+04\n",
       "Date:                Thu, 02 Feb 2023   Prob (F-statistic):                        0.00\n",
       "Time:                        13:32:59   Log-Likelihood:                         -22019.\n",
       "No. Observations:               10000   AIC:                                  4.406e+04\n",
       "Df Residuals:                    9990   BIC:                                  4.413e+04\n",
       "Df Model:                          10                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.0538      0.010      5.392      0.000       0.034       0.073\n",
       "x2            -0.0071      0.014     -0.492      0.623      -0.035       0.021\n",
       "x3             0.0265      0.015      1.799      0.072      -0.002       0.055\n",
       "x4             0.0045      0.015      0.304      0.761      -0.024       0.033\n",
       "x5             0.0253      0.015      1.716      0.086      -0.004       0.054\n",
       "x6             0.0019      0.015      0.127      0.899      -0.027       0.031\n",
       "x7             0.0140      0.015      0.949      0.343      -0.015       0.043\n",
       "x8             0.1031      0.015      7.003      0.000       0.074       0.132\n",
       "x9            -0.2769      0.014    -19.140      0.000      -0.305      -0.249\n",
       "x10            1.0471      0.010    104.831      0.000       1.028       1.067\n",
       "==============================================================================\n",
       "Omnibus:                      166.351   Durbin-Watson:                   2.000\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              329.485\n",
       "Skew:                          -0.035   Prob(JB):                     2.84e-72\n",
       "Kurtosis:                       3.886   Cond. No.                         42.8\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "ols = sm.OLS(y_train, x_train)\n",
    "result = ols.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.285909696369293"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_y = pd.Series(y_train)\n",
    "pd_y.describe()\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.3,  1.5, -0.5, -3.2, -2.1, -2.4, -3.2, -3.8, -3.4,  0.4])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 18:59:26.252891: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-03 18:59:26.695633: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-03 18:59:26.695667: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-03 18:59:27.812762: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-03 18:59:27.812847: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-03 18:59:27.812854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/florian/bachelorarbeit/code/Cross-Silo-FL/datasets/horizontal/weather/muenchen.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>temp</th>\n",
       "      <th>dwpt</th>\n",
       "      <th>rhum</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>wpgt</th>\n",
       "      <th>pres</th>\n",
       "      <th>tsun</th>\n",
       "      <th>coco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1879-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.6</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1879-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1879-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1879-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1879-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  temp  dwpt  rhum  prcp  snow  wdir  wspd  wpgt  pres  tsun  \\\n",
       "0  1879-01-01   NaN   4.6   9.8   0.5   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1  1879-01-02   NaN   0.7   8.5   2.8   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2  1879-01-03   NaN  -0.9   8.6   0.7   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3  1879-01-04   NaN   0.4   8.0   1.0   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4  1879-01-05   NaN  -4.7   0.0   5.9   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   coco  \n",
       "0   NaN  \n",
       "1   NaN  \n",
       "2   NaN  \n",
       "3   NaN  \n",
       "4   NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(path, names=[\"time\", \"temp\", \"dwpt\", \"rhum\", \"prcp\", \"snow\", \"wdir\", \"wspd\", \"wpgt\", \"pres\", \"tsun\", \"coco\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_columns = [\"temp\", \"pres\", \"tsun\"]\n",
    "edog_column = \"temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dataset = tf.data.Dataset.from_tensor_slices(dict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'1985-01-15', b'1985-01-26']\n"
     ]
    }
   ],
   "source": [
    "exog = None\n",
    "edog = None\n",
    "x_train = []\n",
    "y_train = []\n",
    "for window in tf_dataset.batch(11, drop_remainder=True).take(10000):\n",
    "    new_df = pd.DataFrame(window)\n",
    "    exog = new_df[exog_columns].iloc[:10]\n",
    "    edog = new_df[\"time\"].iloc[-1]\n",
    "\n",
    "    if not exog.isnull().values.any() or not edog: #check for NaNs\n",
    "        exog = exog.to_numpy().flatten()\n",
    "\n",
    "\n",
    "        x_train.append(exog)\n",
    "        y_train.append(edog)\n",
    "    #     exog = None\n",
    "    #     edog = None\n",
    "\n",
    "print(y_train[0:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
